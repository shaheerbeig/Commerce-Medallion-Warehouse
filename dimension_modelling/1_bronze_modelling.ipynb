{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7abfbc1-b8b1-4688-a254-f64138af88bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog ecommerce;\n",
    "use schema raw_data;\n",
    "\n",
    "CREATE VOLUME raw_data_vol\n",
    "COMMENT 'Raw CSV files for ecommerce project';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e428e530-ccb4-4dc7-b9fc-3ba27150763c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types  import StructType , StructField , StringType , IntegerType , DataType, TimestampType , FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "brand_schema = StructType([\n",
    "    StructField(\"brand_code\",StringType(),False),\n",
    "    StructField(\"brand_name\",StringType(),False),\n",
    "    StructField(\"category_code\",StringType(),False)\n",
    "])\n",
    "\n",
    "file_location = \"/Volumes/ecommerce/raw_data/raw_data_vol/brands/*.csv\"\n",
    "df = spark.read.option('header',\"true\").format(\"csv\").schema(brand_schema).load(file_location)\n",
    "df = df.withColumn(\"ingested_at\",F.current_timestamp()).withColumn(\"id\",F.uuid())\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.bronze.bronze_base_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b45159-94df-49bc-a1b1-b6b72052b6f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types  import StructType , StructField , StringType , IntegerType , DataType, TimestampType , FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "brand_schema = StructType([\n",
    "    StructField(\"category_name\",StringType(),False),\n",
    "    StructField(\"category_code\",StringType(),False)\n",
    "])\n",
    "\n",
    "file_location = \"/Volumes/ecommerce/raw_data/raw_data_vol/category/*.csv\"\n",
    "df = spark.read.option('header',\"true\").format(\"csv\").schema(brand_schema).load(file_location)\n",
    "df = df.withColumn(\"ingested_at\",F.current_timestamp()).withColumn(\"id\",F.uuid())\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.bronze.category_base_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd873eb0-a71d-4943-938a-df92fd94b7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types  import StructType , StructField , StringType , IntegerType , DataType, TimestampType , FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "brand_schema = StructType([\n",
    "    StructField(\"customer_id\",StringType(),False),\n",
    "    StructField(\"phone\",StringType(),True),\n",
    "    StructField(\"country_code\",StringType(),False),\n",
    "    StructField(\"country\",StringType(),True),\n",
    "    StructField(\"state\",StringType(),True),\n",
    "])\n",
    "\n",
    "file_location = \"/Volumes/ecommerce/raw_data/raw_data_vol/customers/*.csv\"\n",
    "df = spark.read.option('header',\"true\").format(\"csv\").schema(brand_schema).load(file_location)\n",
    "df = df.withColumn(\"ingested_at\",F.current_timestamp()).withColumn(\"id\",F.uuid())\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.bronze.customer_base_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe47602-32fa-4d2d-876c-a5f0329d7409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types  import StructType , StructField , StringType , IntegerType , DataType, TimestampType , FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "brand_schema = StructType([\n",
    "    StructField(\"date\",StringType(),False),\n",
    "    StructField(\"year\",IntegerType(),False),\n",
    "    StructField(\"day_name\",StringType(),False),\n",
    "    StructField(\"quarter\",IntegerType(),False),\n",
    "    StructField(\"week_of_year\",IntegerType(),False),\n",
    "])\n",
    "\n",
    "file_location = \"/Volumes/ecommerce/raw_data/raw_data_vol/date/*.csv\"\n",
    "df = spark.read.option('header',\"true\").format(\"csv\").schema(brand_schema).load(file_location)\n",
    "df = df.withColumn(\"ingested_at\",F.current_timestamp()).withColumn(\"id\",F.uuid())\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.bronze.date_base_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e684fcaf-6ab1-40f7-81fd-c120842a9ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types  import StructType , StructField , StringType , IntegerType , DataType, TimestampType , FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "brand_schema = StructType([\n",
    "    StructField(\"product_id\",StringType(),False),\n",
    "    StructField(\"sku\",StringType(),False),\n",
    "    StructField(\"category_code\",StringType(),False),\n",
    "    StructField(\"brand_code\",StringType(),False),\n",
    "    StructField(\"color\",StringType(),False),\n",
    "    StructField(\"size\",StringType(),False),\n",
    "    StructField(\"material\",StringType(),False),\n",
    "    StructField(\"weight_grams\",StringType(),False),\n",
    "    StructField(\"length_cm\",StringType(),False),\n",
    "    StructField(\"width_cm\",FloatType(),False),\n",
    "    StructField(\"height_cm\",FloatType(),False),\n",
    "    StructField(\"rating_count\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "file_location = \"/Volumes/ecommerce/raw_data/raw_data_vol/products/*.csv\"\n",
    "df = spark.read.option('header',\"true\").format(\"csv\").schema(brand_schema).load(file_location)\n",
    "df = df.withColumn(\"ingested_at\",F.current_timestamp()).withColumn(\"id\",F.uuid())\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.bronze.product_base_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6278044692667856,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_bronze_modelling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
